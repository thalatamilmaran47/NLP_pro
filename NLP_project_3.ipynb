{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODAdmOCBQ3qGJhMepX2ZaZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thalatamilmaran47/NLP_pro/blob/main/NLP_project_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUUiGjA5o0db",
        "outputId": "587fdfd0-ae2c-476c-c9ac-adc2dbd2ed82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Words:\n",
            "younger: 1\n",
            "vulnerable: 1\n",
            "years: 1\n",
            "father: 1\n",
            "gave: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Ensure you have the necessary data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Added to download the missing resource\n",
        "\n",
        "def analyze_word_freq(text):\n",
        "    # 1. Tokenization: Convert text into a list of words\n",
        "    raw_tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # 2. Cleaning: Remove punctuation and common \"stop words\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # We use .isalnum() to keep only alphanumeric tokens (removes symbols)\n",
        "    clean_tokens = [\n",
        "        word for word in raw_tokens\n",
        "        if word.isalnum() and word not in stop_words\n",
        "    ]\n",
        "\n",
        "    # 3. Frequency Calculation\n",
        "    fdist = FreqDist(clean_tokens)\n",
        "\n",
        "    return fdist\n",
        "\n",
        "# Example Text (The beginning of 'The Great Gatsby')\n",
        "text_data = \"\"\"\n",
        "In my younger and more vulnerable years my father gave me some advice\n",
        "that I've been turning over in my mind ever since. Whenever you feel\n",
        "like criticizing anyone, he told me, just remember that all the people\n",
        "in this world haven't had the advantages that you've had.\n",
        "\"\"\"\n",
        "\n",
        "results = analyze_word_freq(text_data)\n",
        "\n",
        "# Print the 5 most common words and their counts\n",
        "print(\"Top 5 Words:\")\n",
        "for word, frequency in results.most_common(5):\n",
        "    print(f\"{word}: {frequency}\")"
      ]
    }
  ]
}